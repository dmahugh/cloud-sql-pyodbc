{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Introduction to pyodbc for Cloud SQL for SQL Server\n",
                "This notebook covers how to get started with pyodbc, including how to connect to a SQL Server instance in Cloud SQL and examples of how to handle some common scenarios for developers and data scientists. You can download a copy of this notebook from the GitHub repo https://github.com/dmahugh/cloud-sql-pyodbc, and more information is available in the blog post [Getting started with pyodbc and Cloud SQL for SQL Server](https://medium.com/@dmahugh_70618/using-pyodbc-with-cloud-sql-for-sql-server-602fb7a1be1b).\n",
                "\n",
                "The snippets below connect to Cloud SQL via the Cloud SQL Proxy listening on 127.0.0.1, so the notebook needs to run on your local machine. Connecting via the proxy is the recommended best practice for local development of Cloud SQL applications.\n",
                "\n",
                "I've tested this notebook on a Windows 10 laptop in [VS Code](https://code.visualstudio.com/docs/python/jupyter-support), [Azure Data Studio](https://docs.microsoft.com/en-us/sql/azure-data-studio/download?view=sql-server-ver15). Anaconda is the \"strongly recommended\" option on the [Jupyter.org install page](https://jupyter.org/install), so start there if you've never worked with notebooks locally."
            ],
            "metadata": {
                "azdata_cell_guid": "87bf6f18-abb2-4298-9992-c24498eee369"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Setup\r\n",
                "Before you can run the code snippets in this notebook, you'll need to go through these one-time setup steps:\r\n",
                "\r\n",
                "* Create a Cloud SQL for SQL Server instance and service account, then configure a local proxy connection as covered in [Cloud SQL Setup](https://medium.com/@dmahugh_70618/cloud-sql-setup-4fc72d3f33db).\r\n",
                "* Clone the [GitHub repo](https://github.com/dmahugh/cloud-sql-pyodbc) to a local folder, and edit the ```config.py``` file to enter your project and Cloud SQL settings. (See the **ODBC Drivers** section below for information about how to set the ```driver``` setting.)\r\n",
                "* Install the ```pyodbc``` package in your notebook environment. The way to do this can vary depending on which notebook software you're running, but in most cases you can simply do a ```!pip install pyodbc``` command in a notebook cell."
            ],
            "metadata": {
                "azdata_cell_guid": "7977217e-fdb2-4f35-a72e-e7ce535fd1b7"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ODBC drivers\n",
                "The pyodbc module communicates with your database through an _ODBC driver_. [ODBC (Open Database Connectivity)](https://en.wikipedia.org/wiki/Open_Database_Connectivity) is an API standard developed in the early 90s by Microsoft and others, with the goal of providing a consistent programming interface for database access that could be used on any platform and from any programming language.\n",
                "\n",
                "The recommended driver for Cloud SQL for SQL Server is [ODBC Driver 17 for SQL Server](https://www.microsoft.com/en-us/download/details.aspx?id=56567). It provides full support for all the features of the SQL Server version currently available on Cloud SQL, SQL Server 2017. For this notebook, you can also use the ```SQL Server Native Client``` driver or even the ```SQL Server``` driver released in 2000 and still found on many Windows machines, but those are deprecated, don't support all of SQL Server 2017's features, and shouldn't be used in production.\n",
                "\n",
                "If you're using Linux or Mac OS for local development, see [Installing the Microsoft ODBC Driver for SQL Server on Linux and macOS](https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver15) to get an ODBC driver for your environment. There are also some good tips and additional information in the [Connecting to SQL Server from Linux](https://github.com/mkleehammer/pyodbc/wiki/Connecting-to-SQL-Server-from-Linux) article on the pyodbc wiki.\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "2e195ecf-54aa-45f7-992e-7d586bc9676d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Example of how to check which ODBC drivers are installed on your system.\n",
                "import pyodbc\n",
                "\n",
                "for driver in pyodbc.drivers():\n",
                "    print(driver)"
            ],
            "metadata": {
                "azdata_cell_guid": "b7d60ecf-3a62-4763-873e-198772afd092"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "SQL Server\nSQL Server Native Client 11.0\nODBC Driver 17 for SQL Server\nMySQL ODBC 8.0 ANSI Driver\nMySQL ODBC 8.0 Unicode Driver\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "ODBC drivers are referenced by name, so copy one of the above names to the ```driver=``` setting in your config.py file. Ideally you'll be using **ODBC Driver 17 for SQL Server** if you have it. This notebook also works fine with the **SQL Server Native Client 11.0** driver, although that one's deprecated and shouldn't be used in production."
            ],
            "metadata": {
                "azdata_cell_guid": "32f60781-4e35-4719-82fe-21f51f93d4e9"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Connection string syntax\n",
                "The code snippet below shows how to construct a Cloud SQL for SQL Server connection string from the settings in ```config.py```."
            ],
            "metadata": {
                "azdata_cell_guid": "24221320-98fe-42a4-bafb-54d058f0ef10"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Example of creating a connection string from settings stored in config.py.\n",
                "\n",
                "import config\n",
                "\n",
                "CONNECTION_STRING = f\"DRIVER={{{config.driver}}};SERVER={config.server};UID={config.sql_user};PWD={config.sql_password}\"\n",
                "\n",
                "print(\" config.py settings: \".center(80, \"-\"))\n",
                "print(f\"      driver = {config.driver}\")\n",
                "print(f\"      server = {config.server}\")\n",
                "print(f\"    sql_user = {config.sql_user}\")\n",
                "print(f\"sql_password = <{len(config.sql_password)} characters>\")\n",
                "print(\" CONNECTION_STRING: \".center(80, \"-\"))\n",
                "before, after = CONNECTION_STRING.split(\"PWD=\")\n",
                "print(before + f\"PWD=<{len(after)} characters>\")"
            ],
            "metadata": {
                "azdata_cell_guid": "a1ab09eb-171f-492d-aca7-55ac08aeb1d4"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "----------------------------- config.py settings: ------------------------------\n      driver = ODBC Driver 17 for SQL Server\n      server = 127.0.0.1\n    sql_user = sqlserver\nsql_password = <16 characters>\n------------------------------ CONNECTION_STRING: ------------------------------\nDRIVER={ODBC Driver 17 for SQL Server};SERVER=127.0.0.1;UID=sqlserver;PWD=<16 characters>\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": [
                "Here are a few things to note about the connection string we've created:\r\n",
                "\r\n",
                "* This connection string doesn't include a ```DATABASE=``` parameter. That's because we're going to connect to a newly created Cloud SQL instance that may not have any databases in it yet. Once a database has been created within the instance, either programmatically as shown below or manually in the Cloud Console, you can include the DATABASE parameter and then you don't need to ```USE <database>``` in your code.\r\n",
                "* We're not using the ```Trusted_Connection=``` parameter either. That's for Windows authentication, and it allows you to omit the ```UID``` and ```PWD``` parameters to authenticate to SQL Server with your current Windows account. (This is not supported in Cloud SQL yet, but it's coming.)\r\n",
                "* The triple braces around ```config.driver``` are necessary because the driver name needs to have braces around it in the connection string. So there are braces around config.driver to embed the driver name, plus double braces around _that_ to put braces around the driver name. (A double brace is how you escape a brace in a Python f-string.)\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "27a39993-5ea7-4b64-912d-7b3db4036769"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Reminder: is the Cloud SQL Proxy running?\n",
                "Now that we have a valid connection string, we're ready to connect to Cloud SQL and execute some SQL commands! And that means the Cloud SQL Proxy needs to be running locally and listening for connections, as shown in the diagram below. If you have questions about how to do that, see [Cloud SQL Setup](https://medium.com/@dmahugh_70618/cloud-sql-setup-4fc72d3f33db).\n",
                "\n",
                "![using the Cloud SQL Proxy](https://github.com/dmahugh/cloud-sql-pyodbc/raw/master/images/notebook_proxy.png)\n",
                "\n",
                "When the proxy is running and connected to Cloud SQL, you'll see this on the console:\n",
                "```\n",
                "YYYY/MM/DD HH:MM:SS using credential file for authentication; email=<SERVICE-ACCOUNT>@<PROJECT>.iam.gserviceaccount.com\n",
                "YYYY/MM/DD HH:MM:SS Listening on 127.0.0.1:1433 for <PROJECT>:<REGION>:<INSTANCE>\n",
                "YYYY/MM/DD HH:MM:SS Ready for new connections\n",
                "```\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "0c6ea65a-db01-490c-a1ca-766265a40498"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Creating a database\n",
                "The first thing we'll do is create a database named ```testdb```, which we'll use for the other snippets to follow.\n",
                "\n",
                "A common issue in creating SQL Server resources such as databases, tables, or users is that you may not know whether that resource already exists. For example, if you have already creates the ```testdb``` database and then try to execute ```CREATE DATABASE testdb``` again, you'll get an error because the database already exists.\n",
                "\n",
                "The idiomatic way to get around that issue varies between database engines and versions. You need to use a different syntax for MySQL than for SQL Server, and older versions of SQL Server require yet another approach.\n",
                "\n",
                "The snippet below demonstrates the recommended approach for creating a database if it doesn't already exist in SQL Server 2016 and above, using the ```DROP DATABASE IF EXISTS``` command."
            ],
            "metadata": {
                "azdata_cell_guid": "f35e7997-04d2-4275-8046-69a101b5820b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Example of how to create an empty database. Any existing database of the same name is dropped first.\n",
                "import pyodbc\n",
                "\n",
                "database = \"testdb\"\n",
                "with pyodbc.connect(CONNECTION_STRING, autocommit=True).cursor() as cursor:\n",
                "    cursor.execute(f\"DROP DATABASE IF EXISTS {database}\")\n",
                "    cursor.execute(f\"CREATE DATABASE {database}\")\n",
                "\n",
                "    # Print the list of databases in the instance, showing that testdb has been created.\n",
                "    cursor.execute(\"SELECT name FROM master.sys.databases\")\n",
                "    for row in cursor:\n",
                "        db_name = row[0]\n",
                "        print(db_name + (\" <<--- CREATED\" if db_name == database else \"\"))\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "8d40425a-14a5-44ec-a3b4-53816dafed10"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "master\ntempdb\nmodel\nmsdb\ntestdb <<--- CREATED\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": [
                "A couple more details to note about the above code:\r\n",
                "* The ```with``` statement creates a cursor from the pyodbc connection as a _context manager_, which is the Python construct for managing limited resources (such as database connections) that need to be allocated and then released. We don't need to explicitly close the connection or cursor, because that happens automatically at the end of the indented ```with``` block.\r\n",
                "* The ```autocommit``` setting determines how database transactions are handled. With ```autocommit=True```, we're foregoing the ability to commit or rollback a multi-statement transaction, and instead each SQL command passed to ```cursor.execute()``` is committed to the database immediately. You _must_ do this for a ```CREATE DATABASE``` command, which is not allowed within a multi-statement transaction.\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "0f8a8b42-5f33-4318-a01a-bee59703d8f9"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Getting some sample data\r\n",
                "Let's get some data to use for exploring a few scenarios. The code below downloads a copy of all active [dog licenses in New York City](https://data.cityofnewyork.us/api/views/nu7n-tubp/rows.csv?accessType=DOWNLOAD) and saves them as a CSV file."
            ],
            "metadata": {
                "azdata_cell_guid": "c1efb868-70cd-41b2-81d1-3d0ca6ec964a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import pathlib\r\n",
                "import requests\r\n",
                "\r\n",
                "filename = \"NYC_Dog_Licensing_Dataset.csv\"\r\n",
                "dataset = \"https://data.cityofnewyork.us/api/views/nu7n-tubp/rows.csv?accessType=DOWNLOAD\"\r\n",
                "\r\n",
                "if pathlib.Path(filename).is_file():\r\n",
                "    print(f\"{filename} already exists, not downloaded again.\")\r\n",
                "else:\r\n",
                "    response = requests.get(dataset)\r\n",
                "    with open(filename, \"w\") as csvfile:\r\n",
                "        csvfile.write(response.text)\r\n",
                "    print(f\"{filename} downloaded, {len(response.text)} total bytes\")"
            ],
            "metadata": {
                "azdata_cell_guid": "f36d1663-ae5e-4587-8ded-36858a3f2ff6"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "NYC_Dog_Licensing_Dataset.csv downloaded, 24083896 total bytes\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's take a peek at the data to see how it's structured. A simple way to do this with a CSV file is to read it into a Pandas dataframe and then use the ```head()``` method to display the first 5 rows."
            ],
            "metadata": {
                "azdata_cell_guid": "25b5aadf-8f13-4172-83e0-027063d4d87a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\r\n",
                "\r\n",
                "df = pd.read_csv(filename, encoding=\"ISO-8859-1\")\r\n",
                "print(df.head())"
            ],
            "metadata": {
                "azdata_cell_guid": "aa1300e1-b88a-4dbf-bac2-638e17373507"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "   RowNumber AnimalName AnimalGender  AnimalBirthMonth  \\\n0          1      PAIGE            F              2014   \n1          2       YOGI            M              2010   \n2          3        ALI            M              2014   \n3          4      QUEEN            F              2013   \n4          5       LOLA            F              2009   \n\n                              BreedName  Borough  ZipCode LicenseIssuedDate  \\\n0  American Pit Bull Mix / Pit Bull Mix      NaN    10035        09/12/2014   \n1                                 Boxer      NaN    10465        09/12/2014   \n2                               Basenji      NaN    10013        09/12/2014   \n3                      Akita Crossbreed      NaN    10013        09/12/2014   \n4                               Maltese      NaN    10028        09/12/2014   \n\n  LicenseExpiredDate  Extract Year  \n0         09/12/2017          2016  \n1         10/02/2017          2016  \n2         09/12/2019          2016  \n3         09/12/2017          2016  \n4         10/09/2017          2016  \n"
                }
            ],
            "execution_count": 17
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Creating a table\r\n",
                "Next we'll create a ```samoyed``` table, where we'll store some information about Samoyeds in the dog license data set."
            ],
            "metadata": {
                "azdata_cell_guid": "c8da2f13-c947-496f-8dc0-f2f33c01c258"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "CONNECTION_STRING = f\"DRIVER={{{config.driver}}};SERVER={config.server};DATABASE={database};UID={config.sql_user};PWD={config.sql_password}\"\r\n",
                "\r\n",
                "table = \"samoyed\"\r\n",
                "with pyodbc.connect(CONNECTION_STRING, autocommit=True).cursor() as cursor:\r\n",
                "    cursor.execute(f\"DROP TABLE IF EXISTS {table}\")\r\n",
                "    cursor.execute(f\"\"\"CREATE TABLE {table} (\r\n",
                "        name VARCHAR(50),\r\n",
                "        gender CHAR(1),\r\n",
                "        yearborn CHAR(4),\r\n",
                "        zipcode CHAR(5),\r\n",
                "        licensed DATE)\"\"\")\r\n",
                "\r\n",
                "    # To confirm creation, print out all tables in our new database that are in the default dbo schema.\r\n",
                "    for dbname, schema, tablename, *_ in cursor.tables():\r\n",
                "        if dbname == database and schema == \"dbo\":\r\n",
                "            print(tablename + (\" <<--- CREATED\" if tablename == table else \"\"))"
            ],
            "metadata": {
                "azdata_cell_guid": "578c84aa-c150-45dc-a469-7cca3339dabd"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "samoyed <<--- CREATED\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 12
        },
        {
            "cell_type": "markdown",
            "source": [
                "Note that the connection string in the snippet above has a ```DATABASE=``` parameter to specify the new database we created above. If we used a connection string without that parameter, we'd be connecting to the SQL Server _instance_ in Cloud SQL and then our T-SQL code would need to always include a ```USE <dtabase>``` command to open the proper database."
            ],
            "metadata": {
                "azdata_cell_guid": "b51ccd2c-f1b4-4e4c-ad14-6aceb933fe13"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Adding rows to the table\r\n",
                "We've downloaded a dataset (CSV file) and created a table in Cloud SQL, next we'll use the ```csv``` module from the Python standard to iterate through the CSV file and insert records into our table. We'll just grab the first 20 Samoyed dogs registered in NYC and put them in our ```samoyed``` table.\r\n",
                "\r\n",
                "Our goal here is to learn a few things about working with pyodbc, so we're writing our own code to handle data migration. But you don't need to write your own migration code if you want to migrate an entire database into a Cloud SQL for SQL Server instance. In that scenario, the simpler and faster approach is to use the built-in import functionality described in [Importing data into Cloud SQL](https://cloud.google.com/sql/docs/sqlserver/import-export/importing).\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "e0aaddb5-4283-405d-bc4f-08ab01e9ff60"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import csv\r\n",
                "\r\n",
                "rows_added = 0\r\n",
                "with pyodbc.connect(CONNECTION_STRING, autocommit=True).cursor() as cursor:\r\n",
                "    with open(filename, encoding=\"ISO-8859-1\") as csvfile:\r\n",
                "        csvreader = csv.reader(csvfile)\r\n",
                "        next(csvreader) # skip the header row\r\n",
                "        for row in csvreader:\r\n",
                "            _, name, gender, yearborn, breed, _, zipcode, licensed, *_ = row\r\n",
                "            if breed == \"Samoyed\":\r\n",
                "                values = [name, gender, yearborn, zipcode, licensed]\r\n",
                "                cursor.execute(\"INSERT INTO samoyed (name, gender, yearborn, zipcode, licensed) VALUES (?, ?, ?, ?, ?)\", values)\r\n",
                "                rows_added += 1\r\n",
                "                if rows_added >= 20:\r\n",
                "                    break\r\n",
                "\r\n",
                "print(f\"{rows_added} rows added to the samoyed table\")"
            ],
            "metadata": {
                "azdata_cell_guid": "e9e5822e-20b4-48be-baa0-47e43380d52e"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "20 rows added to the samoyed table\n"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Querying the database\r\n",
                "Now that we have a table with data in it, let's try querying our data with a SELECT command."
            ],
            "metadata": {
                "azdata_cell_guid": "b4bfb82f-d6f0-4818-ba06-8baec3567354"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Simple query example.\r\n",
                "with pyodbc.connect(CONNECTION_STRING).cursor() as cursor:\r\n",
                "    cursor.execute(\"SELECT * FROM samoyed ORDER by zipcode\")\r\n",
                "    for row in cursor:\r\n",
                "        print(f\"zipcode: {row.zipcode}, gender: {row.gender}, name: {row.name}\")"
            ],
            "metadata": {
                "azdata_cell_guid": "40646abc-7b3b-4de1-a11e-e53e08e8637b"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "zipcode: 10009, gender: M, name: KAI\nzipcode: 10009, gender: F, name: LAIKA\nzipcode: 10009, gender: F, name: YUKI\nzipcode: 10013, gender: M, name: OLIVER\nzipcode: 10013, gender: M, name: BEAR\nzipcode: 10013, gender: F, name: POPPY\nzipcode: 10016, gender: M, name: LUX\nzipcode: 10023, gender: F, name: BALTI\nzipcode: 10023, gender: F, name: HATTIE\nzipcode: 10024, gender: F, name: KIKI\nzipcode: 10025, gender: M, name: BARON\nzipcode: 10025, gender: M, name: BISCUIT\nzipcode: 10025, gender: F, name: ALLIE\nzipcode: 10065, gender: F, name: VARESSA\nzipcode: 10128, gender: M, name: SAMMY\nzipcode: 10453, gender: F, name: MARSHMELLO\nzipcode: 11207, gender: M, name: SAM\nzipcode: 11212, gender: M, name: SAMSON\nzipcode: 11228, gender: M, name: LUKA\nzipcode: 11234, gender: M, name: PADDY\n"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "As shown above, the results of a ```SELECT``` query can be accessed by simply iterating through the cursor itself. There are also several convenience methods to make it easy to work with result sets, such as ```cursor.fetchone()``` or ```cursor.fetchall()``` &mdash; see [Select Basics](https://github.com/mkleehammer/pyodbc/wiki/Getting-started#select-basics) in the pyodbc documentation.\r\n",
                "\r\n",
                "Note that we don't have the optional ```automcommit=True``` connection parameter in this snippet. Since we're just reading data with a SELECT, we don't need to worry about committing transactions."
            ],
            "metadata": {
                "azdata_cell_guid": "e474ab2d-f7d6-4b3f-9090-99cc7a145c45"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Protecting from SQL injection attacks\r\n",
                "This has been a quick intro to pyodbc basics, so I've tried to keep things as simple and clear as possible. That means no error handling, and we've also not bothered to [parameterize](https://github.com/mkleehammer/pyodbc/wiki/Getting-started#parameters) our SQL commands.\r\n",
                "\r\n",
                "But if you're working with any form of user inputs or external data sources for the content of your SQL command,  **parameterize your SQL commands to prevent SQL injection attacks!** Don't let [Bobby Tables](https://xkcd.com/327/) mess with your data.\r\n",
                "\r\n",
                "ODBC supports the use of a ```?``` placeholder in a SQL command, which will be replaced at runtime by the value of a separate argument passed to the ```cursor.execute()```. The snippet below shows how to use this feature to run a parameterized query that is protected from SQL injection attacks. For more information see the [pyodbc documentation](https://github.com/mkleehammer/pyodbc/wiki/Getting-started#parameters)."
            ],
            "metadata": {
                "azdata_cell_guid": "f11a6177-7db8-4d51-b0d5-7a7c6806d9ed"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Example of parameterized query to prevent SQL injection attacks.\r\n",
                "\r\n",
                "# This value is hard-coded, but imagine a scenario where we had a form on a web page\r\n",
                "# where a person can enter the ZIPCODE to be queried. In that case, we need to\r\n",
                "# use parameterization to protect from the person entering a \"zip code\" that\r\n",
                "# includes a SQL command..\r\n",
                "ZIPCODE = \"10025\"\r\n",
                "\r\n",
                "with pyodbc.connect(CONNECTION_STRING).cursor() as cursor:\r\n",
                "    cursor.execute(f\"SELECT * FROM samoyed WHERE zipcode = ? ORDER by zipcode\", ZIPCODE)\r\n",
                "    for row in cursor:\r\n",
                "        print(f\"zipcode: {row.zipcode}, gender: {row.gender}, name: {row.name}\")"
            ],
            "metadata": {
                "azdata_cell_guid": "aa0ed7ef-cbd8-4344-b5c2-05f301d14610"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "zipcode: 10025, gender: M, name: BARON\nzipcode: 10025, gender: M, name: BISCUIT\nzipcode: 10025, gender: F, name: ALLIE\n"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Exporting to a Pandas dataframe\r\n",
                "\r\n",
                "If you're a data scientist accustomed to working in Pandas, you'll probably want to get the results of your SQL queries into Pandas dataframes. Pandas provides a ```read_sql()``` function for this, which takes a SQL query and a database connection and returns a dataframe containing the results of the query. The snippet below shows how to use it."
            ],
            "metadata": {
                "azdata_cell_guid": "e7e29a72-23c3-46f7-96ec-50b7f6ee000f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Example of how to put the results of a SQL SELECT into a Pandas dataframe.\r\n",
                "import pandas as pd\r\n",
                "\r\n",
                "database = \"testdb\" # For this example, we'll connect to the database we created earlier.\r\n",
                "CONNECTION_STRING = f\"DRIVER={{{config.driver}}};SERVER={config.server};DATABASE={database};UID={config.sql_user};PWD={config.sql_password}\"\r\n",
                "\r\n",
                "with pyodbc.connect(CONNECTION_STRING) as cnxn:\r\n",
                "    df = pd.read_sql(\"SELECT * FROM samoyed;\", cnxn)\r\n",
                "    print(df.head())\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "f508bda4-1628-4aa2-99bf-6a5f33657f7a"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "         name gender yearborn zipcode    licensed\n0        LUKA      M     2009   11228  2014-12-15\n1        BEAR      M     2014   10013  2015-01-12\n2     VARESSA      F     2014   10065  2015-02-27\n3       POPPY      F     2012   10013  2015-03-03\n4  MARSHMELLO      F     2004   10453  2015-03-27\n"
                }
            ],
            "execution_count": 18
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Conclusions\r\n",
                "in this notebook, we've learned how to configure a pyodobc connection to a Cloud SQL for SQL Server instance, and then we created a database and table, added some data from a public dataset, and did a few queries. We also learned how to protect our code from SQL injection attacks, and how to get SQL data into Pandas dataframes.\r\n",
                "\r\n",
                "There is much more to pyodbc, of course. The documentation on the [pyodbc wiki](https://github.com/mkleehammer/pyodbc/wiki) is excellent, so that's a great place to start if you want to learn more!"
            ],
            "metadata": {
                "azdata_cell_guid": "43bf2461-6be1-4bd9-9047-65c0853d5c5f"
            }
        }
    ]
}